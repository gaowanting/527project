{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP4nFnomJ99E5iUq1WP1q9I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BY2lkrX2l3hQ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","import math\n","import numpy as np\n","from common.memory import ReplayBuffer\n","from common.model import MLP\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["from typing import Any, List, Tuple\n","import numpy as np\n","import pandas as pd\n","import random\n","import gym\n","import time\n","from gym import spaces\n","from sklearn import preprocessing"],"metadata":{"id":"24iIlUHuqx1N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Deep Q Networks Algorithm"],"metadata":{"id":"n-6lX4yfqb-T"}},{"cell_type":"code","source":["class DQN:\n","    def __init__(self, state_dim, action_dim, cfg):\n","\n","        self.action_dim = action_dim  # total number of actions\n","        self.device = cfg.device  # computing device, cpu or gpu\n","        self.gamma = cfg.gamma  # discount factor for rewards\n","        # Parameters for epsilon-greedy policy\n","        self.frame_idx = 0  # counter used for epsilon decay\n","        self.epsilon = lambda frame_idx: cfg.epsilon_end + \\\n","            (cfg.epsilon_start - cfg.epsilon_end) * \\\n","            math.exp(-1. * frame_idx / cfg.epsilon_decay)\n","        self.batch_size = cfg.batch_size\n","        self.policy_net = MLP(state_dim, action_dim, hidden_dim=cfg.hidden_dim).to(self.device)\n","        self.target_net = MLP(state_dim, action_dim, hidden_dim=cfg.hidden_dim).to(self.device)\n","        # Copy parameters from policy network to target network\n","        for target_param, param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n","            target_param.data.copy_(param.data)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=cfg.lr)\n","        self.memory = ReplayBuffer(cfg.memory_capacity)\n","\n","    def choose_action(self, state):\n","        '''Choose an action'''\n","        self.frame_idx += 1\n","        if random.random() > self.epsilon(self.frame_idx):\n","            action = self.predict(state)\n","        else:\n","            action = random.randrange(self.action_dim)\n","        return action\n","\n","    def predict(self, state):\n","        # No gradient computation required\n","        with torch.no_grad():\n","            state = torch.tensor([state], device=self.device, dtype=torch.float32)\n","            q_values = self.policy_net(state)\n","            action = q_values[0][0].tolist().index(q_values[0][0].max())\n","            # action = q_values.max(1)[1].item()\n","        return action\n","\n","    def update(self):\n","        if len(self.memory) < self.batch_size:\n","            return\n","        # Randomly sample transitions from replay memory\n","        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(\n","            self.batch_size)\n","        state_batch = [np.squeeze(i.T) for i in state_batch]\n","        '''Convert to tensor\n","        e.g., tensor([[-4.5543e-02, -2.3910e-01,  1.8344e-02,  2.3158e-01],...,[-1.8615e-02, -2.3921e-01, -1.1791e-02,  2.3400e-01]])'''\n","        state_batch = torch.tensor(\n","            state_batch, device=self.device, dtype=torch.float)\n","        action_batch = torch.tensor(action_batch, device=self.device).unsqueeze(\n","            1)  # e.g., tensor([[1],...,[0]])\n","        reward_batch = torch.tensor(\n","            reward_batch, device=self.device, dtype=torch.float)  # tensor([1., 1.,...,1])\n","        next_state_batch = torch.tensor(\n","            next_state_batch, device=self.device, dtype=torch.float)\n","        done_batch = torch.tensor(np.float32(\n","            done_batch), device=self.device)\n","\n","        '''Calculate Q(s_t, a) for current (s_t,a)'''\n","        '''torch.gather example: a=torch.Tensor([[1,2],[3,4]]), a.gather(1,torch.Tensor([[0],[1]]))=torch.Tensor([[1],[3]])'''\n","        q_values = self.policy_net(state_batch).gather(\n","            dim=1, index=action_batch)  # equivalent to self.forward\n","\n","        # Compute V(s_{t+1}) for all next states, picking max reward from target_net\n","        next_q_values = self.target_net(next_state_batch)[0].max(\n","            1)[0].detach()  # e.g., tensor([ 0.0060, -0.0171,...,])\n","        # Calculate expected Q values\n","        # For terminal states (done_batch[0]=1), expected_q_value equals reward\n","        expected_q_values = reward_batch + \\\n","            self.gamma * next_q_values * (1-done_batch)\n","\n","        # Calculate mean squared error loss\n","        loss = nn.MSELoss()(q_values, expected_q_values.unsqueeze(1))\n","\n","        # Optimize model\n","        self.optimizer.zero_grad()  # Clear old gradients\n","        loss.backward()  # Compute gradients through backpropagation\n","        # Clip gradients to prevent exploding (optional)\n","        # for param in self.policy_net.parameters():\n","        #     param.grad.data.clamp_(-1, 1)\n","        self.optimizer.step()  # Update model parameters\n","\n","    def save(self, path):\n","        torch.save(self.target_net.state_dict(), path+'dqn_checkpoint.pth')\n","        torch.save(self.target_net, path+'full_dqn_model.pth')\n","\n","    def load(self, path):\n","        self.target_net.load_state_dict(torch.load(path+'dqn_checkpoint.pth'))\n","        for target_param, param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n","            param.data.copy_(target_param.data)\n"],"metadata":{"id":"9IKxcOSUpdnk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Stock Market Environment"],"metadata":{"id":"9xsYN-jhqy2X"}},{"cell_type":"code","source":["class StockLearningEnv(gym.Env):\n","\n","    def render(self, mode=\"human\"):\n","        pass\n","\n","\n","    def __init__(\n","            self,\n","            df: pd.DataFrame,\n","            buy_cost_pct: float = 3e-3,\n","            sell_cost_pct: float = 3e-3,\n","            print_verbosity: int = 10,\n","            initial_amount: int = 1e6,\n","            patient: bool = False,\n","            currency: str = \"￥\",\n","            is_train: bool = True,\n","    ) -> None:\n","\n","        self.df = df\n","        self.dates = df['date']\n","        self.date_index = 0\n","        self.df = self.df.set_index('date')\n","        self.assets = df['tic']\n","        self.patient = patient\n","        self.currency = currency\n","        self.is_train = is_train\n","        self.initial_amount = initial_amount\n","        self.print_verbosity = print_verbosity\n","        self.buy_cost_pct = buy_cost_pct\n","        self.sell_cost_pct = sell_cost_pct\n","        self.window_size = 20\n","        self.state_list = self.state\n","        self.state_space = len(self.state_list)*self.window_size\n","        self.action_space = spaces.Discrete(3)\n","        self.observation_space = spaces.Box(\n","            low=-np.inf, high=np.inf, shape=(self.state_space,)\n","        )\n","        self.seed()\n","        self.episode = -1\n","        self.episode_history = []\n","        self.printed_header = False\n","        self.max_total_assets = 0\n","        self.account_information = {\n","            \"cash\": [],\n","            \"asset_value\": [],\n","            \"total_assets\": [],\n","            \"reward\": []\n","        }\n","        self.rolling_window = True\n","        self.normalization = 'div_self' # choose thw way to process normalization ('div_self' / 'div_close' / 'standardization')\n","        self.do_normalization()\n","\n","    def reset(self) -> np.ndarray:\n","        self.seed()\n","        self.max_total_assets = self.initial_amount\n","        self.starting_point = 0\n","        self.date_index = self.starting_point\n","        self.episode += 1\n","        self.actions_memory = []\n","        self.transaction_memory = []\n","        self.state_memory = []\n","        self.coh_memory = [1e+6]\n","        self.holdings_memory = [0]\n","        self.account_information = {\n","            \"cash\": [],\n","            \"asset_value\": [],\n","            \"total_assets\": [],\n","            \"reward\": []\n","        }\n","        init_state = np.zeros(self.state_space)\n","        self.state_memory.append(init_state)\n","        return np.array([init_state])\n","\n","    def step(\n","            self, actions: np.ndarray\n","    ) -> Tuple[list, float, bool, dict]:\n","        self.log_header()\n","        if (self.current_step + 1) % self.print_verbosity == 0:\n","            self.log_step(reason=\"update\")\n","        # save evaluate information on last step for each episode\n","        if self.date_index == len(self.dates) - 1:\n","            if self.is_train:\n","                save_path = f\"train_record/train_action{self.episode}.csv\"\n","                self.save_transaction_information().to_csv(save_path)\n","            return self.return_terminal(reward=self.reward)\n","        else:\n","            self.action = actions - 1\n","            transactions = self.action * 1000\n","            begin_cash = self.cash_on_hand\n","            assert_value = np.dot(self.holdings, self.closings)  # total assets\n","            reward = self.reward\n","            #save account_information\n","            self.account_information[\"cash\"].append(begin_cash)\n","            self.account_information[\"asset_value\"].append(assert_value)\n","            self.account_information[\"total_assets\"].append(begin_cash + assert_value)\n","            self.account_information[\"reward\"].append(reward)\n","            self.actions_memory.append(self.action)\n","            self.transaction_memory.append(transactions)\n","\n","            sells = -np.clip(transactions, -np.inf, 0)\n","            proceeds = np.dot(sells, self.closings)\n","            costs = proceeds * self.sell_cost_pct\n","            coh = begin_cash + proceeds  # calculate current cash\n","            buys = np.clip(transactions, 0, np.inf)\n","            spend = np.dot(buys, self.closings)\n","            costs += spend * self.buy_cost_pct\n","            coh = coh - spend - costs\n","            holdings_updated = self.holdings + transactions\n","            self.date_index += 1\n","\n","            # do standardization in a sliding window\n","            if self.normalization == \"standardization\":\n","                temp = self.df.loc[\n","                        self.df.index[self.date_index]:self.df.index[self.date_index + self.window_size - 1],\n","                        [\"open\", \"close\", \"high\", \"low\", \"volume\"]]\n","                scaler = preprocessing.StandardScaler().fit(temp)\n","                self.df.loc[self.df.index[self.date_index]:self.df.index[self.date_index + self.window_size - 1],\n","                [\"open_\", \"close_\", \"high_\", \"low_\", \"volume_\"]] = scaler.transform(temp)\n","            state = self.df.loc[self.df.index[self.date_index]:self.df.index[self.date_index + self.window_size - 1],\n","                    self.state_list]\n","            state = state.values.reshape(1,220)\n","            self.state_memory.append(state)\n","            self.coh_memory.append(coh)\n","            self.holdings_memory.append(holdings_updated)\n","            return state, reward, False, {}\n","\n","\n","    def seed(self, seed: Any = None) -> None:\n","        \"\"\"random seed\"\"\"\n","        if seed is None:\n","            seed = int(round(time.time() * 1000))\n","        random.seed(seed)\n","\n","    def log_step(\n","            self, reason: str, terminal_reward: float = None\n","    ) -> None:\n","        \"\"\"print\"\"\"\n","        if terminal_reward is None:\n","            terminal_reward = self.account_information[\"reward\"][-1]\n","        assets = self.account_information[\"total_assets\"][-1]\n","        gl_pct = self.account_information[\"total_assets\"][-1] / self.initial_amount  # GAINLOSS_PCT\n","\n","        rec = [\n","            self.episode,\n","            self.date_index - self.starting_point,\n","            reason,\n","            f\"{self.currency}{'{:0,.0f}'.format(float(self.account_information['cash'][-1]))}\",\n","            f\"{self.currency}{'{:0,.0f}'.format(float(assets))}\",\n","            f\"{terminal_reward * 100:0.5f}%\",\n","            f\"{(gl_pct - 1) * 100:0.5f}%\",\n","        ]\n","        self.episode_history.append(rec)\n","        print(self.template.format(*rec))\n","\n","    def return_terminal(\n","            self, reason: str = \"Last Date\", reward: int = 0\n","    ) -> Tuple[list, int, bool, dict]:\n","        \"\"\"terminal\"\"\"\n","        state = self.state_memory[-1]\n","        self.log_step(reason=reason, terminal_reward=reward)\n","        gl_pct = self.account_information[\"total_assets\"][-1] / self.initial_amount\n","        reward_pct = gl_pct\n","        return state, reward, True, {}\n","\n","    def log_header(self) -> None:\n","        \"\"\"Log column name\"\"\"\n","        if not self.printed_header:\n","            self.template = \"{0:4}|{1:4}|{2:15}|{3:15}|{4:15}|{5:10}|{6:10}\"\n","            # 0, 1, 2, ... 是序号\n","            # 4, 4, 15, ... 是占位格的大小\n","            print(\n","                self.template.format(\n","                    \"EPISODE\",\n","                    \"STEPS\",\n","                    \"TERMINAL_REASON\",\n","                    \"CASH\",\n","                    \"TOT_ASSETS\",\n","                    \"TERMINAL_REWARD\",\n","                    \"GAINLOSS_PCT\",\n","                )\n","            )\n","            self.printed_header = True\n","\n","    def save_transaction_information(self) -> pd.DataFrame:\n","        if self.current_step == 0:\n","            return None\n","        else:\n","            action_df = pd.DataFrame(\n","                {\n","                    \"close\": self.df[\"close\"][-len(self.account_information[\"cash\"]):],\n","                    \"episode\": self.episode,\n","                    \"actions\": self.actions_memory,\n","                    \"transactions\": self.transaction_memory,\n","                    \"total_assets\": self.account_information[\"total_assets\"],\n","                    \"reward\": self.reward,\n","                    \"assets_baseline\": self.assets_baseline()\n","                })\n","            return action_df\n","\n","    # buy and hold\n","    def assets_baseline(self):\n","        # Based on the amount of assets that are no longer traded after the first full-position purchase of stocks,\n","        # reflecting the changes in individual stocks themselves\n","        close = self.df[\"close\"][0]\n","        initial_assets = int(self.account_information[\"total_assets\"][0] / (close * 100)) * 100\n","        return self.df[\"close\"][-len(self.account_information[\"cash\"]):] * initial_assets\n","\n","    @property\n","    def current_step(self) -> int:\n","        return self.date_index - self.starting_point\n","\n","    @property\n","    def cash_on_hand(self) -> float:\n","        coh = self.coh_memory[-1]\n","        return coh\n","\n","    @property\n","    def holdings(self) -> List:\n","        holdings = self.holdings_memory[-1]\n","        return holdings\n","\n","    @property\n","    def closings(self) -> List:\"\n","        close = self.df.loc[self.df.index[self.date_index], \"close\"]\n","        return close\n","\n","    @property\n","    def state(self):\n","        state1 = ['kdjk', 'kdjd', 'kdjj', 'rsi_6', 'rsi_12', 'rsi_24']\n","        state2 = ['kdjk', 'kdjd', 'kdjj', 'rsi_6', 'rsi_12', 'rsi_24', \"open_\", \"close_\", \"high_\", \"low_\", \"volume_\"]\n","        return state2\n","\n","\n","    @property\n","    def reward(self) -> float:\n","        # init\n","        epsilon = 0.001\n","        signal_dict = {'-':0,\"v\":-1,\"^\":1}\n","        current_date = self.df.index[self.date_index]\n","        signal = signal_dict.get(self.df.loc[self.df.index[self.date_index],'landmark'])\n","        # get y\n","        y_current = self.closings\n","        y_valley = self.df[(self.df.landmark == 'v') & (self.df.index > current_date)].iloc[0,4]\n","        y_peak = self.df[(self.df.landmark == '^') & (self.df.index > current_date)].iloc[0,4]\n","        y_valley_date = self.df[(self.df.landmark == 'v') & (self.df.index > current_date)].index[0]\n","        y_peak_date = self.df[(self.df.landmark == '^') & (self.df.index > current_date)].index[0]\n","        # calculate reward\n","        if self.action != 0:\n","            reward = (y_peak - y_valley)/(y_peak - y_current + epsilon) if (y_peak_date > y_valley_date) else (y_peak - y_valley)/(y_current - y_valley + epsilon)\n","            if signal != self.action:\n","                reward = -reward\n","        else:\n","            reward = 0\n","        print(np.tanh(reward))\n","        return np.tanh(reward)"],"metadata":{"id":"C-nK1Q6DqUBl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(r\"apple_preprocessed_data.csv\")\n","s = StockLearningEnv(df)\n","print(s.reward)"],"metadata":{"id":"NN1urY_JrdTu"},"execution_count":null,"outputs":[]}]}